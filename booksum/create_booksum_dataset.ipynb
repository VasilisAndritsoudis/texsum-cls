{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BookSum Dataset Creation\n",
    "The produced Dataset will have the following structure:\n",
    "* Each document (text+abstract) entry is saved in a different file (e.g. data_{i}.txt)\n",
    "* Each document has the following structure: [CLS] text [SEP] abstract [SEP]\n",
    "* The labels of the documents are saved in a single file (e.g. labels.txt), where each line corresponds to the document with id equal to the line number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12630"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['alignments/chapter-level-summary-alignments/chapter_summary_aligned_train_split.jsonl',\n",
    "            'alignments/chapter-level-summary-alignments/chapter_summary_aligned_test_split.jsonl',\n",
    "            'alignments/chapter-level-summary-alignments/chapter_summary_aligned_val_split.jsonl']\n",
    "alignments = []\n",
    "for json_file_path in paths:\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            alignment = json.loads(line)\n",
    "            alignments.append(alignment)\n",
    "len(alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_docs = len(alignments)  # 12630\n",
    "num_neg_ex = 2\n",
    "\n",
    "pos_label = \"1\"\n",
    "neg_label = \"0\"\n",
    "\n",
    "doc_len = 100000000#2000\n",
    "abst_len = 1000000#500\n",
    "\n",
    "test_docs_per = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_characteristics = \"whole_text\"\n",
    "train_path = f\"data/booksum/data_{dataset_characteristics}/raw/train/\"\n",
    "test_path  = f\"data/booksum/data_{dataset_characteristics}/raw/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in word_list if word.lower() not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, seq_length):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        words = data.split(\" \")\n",
    "        words = remove_stopwords(words)\n",
    "        text = ' '.join((words[:seq_length])) if len(words) > seq_length else ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def read_json_file(path, seq_length):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        json_string = f.read()\n",
    "        data = json.loads(json_string)\n",
    "\n",
    "        f.close()\n",
    "        words = data[\"summary\"].split(\" \")\n",
    "        words = remove_stopwords(words)\n",
    "        text = ' '.join((words[:seq_length])) if len(words) > seq_length else ' '.join(words)\n",
    "    return text\n",
    "    \n",
    "def write_file(path, data):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset construction containing raw data\n",
    "The BookSum Data is split into train and test sets.\n",
    "\n",
    "Each Dataset is constructed as follows:\n",
    "1. Each document is written to a separate file.\n",
    "2. The text and abstract are truncated in order to match the desired lengths.\n",
    "3. Each document is constructed by concatenating the text and abstract of a paper using special separators ([CLS], [SEP]).\n",
    "4. Negative examples are produced by picking random abstracts of other documents and creating a new negative entry as described above.\n",
    "5. Each positive example is copied as many times as needed in order to balance the positive and negative examples.\n",
    "6. The labels are written into a single document, where each line corresponds to the document with id equal to the line number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12630 alignment pairs (chapter-summary) but we do not have summaries for all chapters due to limitation in scraping the provided links. We have gathered 4843 chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all summary paths for negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4843"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_a_path = 'scripts/finished_summaries'\n",
    "all_file_names = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_a_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            all_file_names.append(file_path)\n",
    "len(all_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630\n",
      "4145\n",
      "8485\n"
     ]
    }
   ],
   "source": [
    "clear_alignments = []\n",
    "summaries_not_available = 0\n",
    "for alignment in alignments:\n",
    "    summary_file = \"scripts/\" + alignment[\"summary_path\"]\n",
    "    summary_file = summary_file.replace(':','_')\n",
    "    try:\n",
    "        with open(summary_file, encoding=\"utf8\") as f:\n",
    "            json_string = f.read()\n",
    "    except FileNotFoundError:\n",
    "        summaries_not_available += 1\n",
    "        continue\n",
    "    clear_alignments.append(alignment)\n",
    "\n",
    "print(len(alignments))\n",
    "print(len(clear_alignments))    # some summaries refer to the same chapter\n",
    "print(summaries_not_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(clear_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = 1\n",
    "doc_labels = []\n",
    "test_start = int(num_docs * (1 - test_docs_per)) + 1\n",
    "\n",
    "for alignment in clear_alignments[:test_start]:\n",
    "    chapter_file = alignment[\"chapter_path\"]\n",
    "    summary_file = \"scripts\\\\\" + alignment[\"summary_path\"]\n",
    "\n",
    "    chapter_file = chapter_file.replace(':','_')    # ':' is an invalid character for filename\n",
    "    summary_file = summary_file.replace(':','_')\n",
    "\n",
    "    text = read_file(chapter_file, doc_len)\n",
    "    try:\n",
    "        abst = read_json_file(summary_file, abst_len)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    \n",
    "    doc_data = \"[CLS] \" + text + \" [SEP] \" + abst + \" [SEP]\"\n",
    "    doc_label = pos_label\n",
    "\n",
    "    for j in range(num_neg_ex):\n",
    "        # Write positive example\n",
    "        write_file(train_path + f\"data_{doc_count}.txt\", doc_data)\n",
    "        doc_labels.append(doc_label)\n",
    "        doc_count += 1\n",
    "        \n",
    "        # Write negative example\n",
    "        random_element = random.choice(all_file_names)\n",
    "        while random_element == summary_file:\n",
    "            random_element = random.choice(all_file_names)\n",
    "\n",
    "        neg_ex_abst = read_json_file(random_element, abst_len)\n",
    "        neg_ex_data = \"[CLS] \" + text + \" [SEP] \" + neg_ex_abst + \" [SEP]\"\n",
    "        write_file(train_path + f\"data_{doc_count}.txt\", neg_ex_data)\n",
    "        doc_labels.append(neg_label)\n",
    "        doc_count += 1\n",
    "\n",
    "write_file(train_path + f\"labels.txt\", \"\\n\".join(doc_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = 1\n",
    "doc_labels = []\n",
    "\n",
    "for alignment in clear_alignments[test_start:]:\n",
    "    chapter_file = alignment[\"chapter_path\"]\n",
    "    summary_file = \"scripts/\" + alignment[\"summary_path\"]\n",
    "\n",
    "    chapter_file = chapter_file.replace(':','_')    # ':' is an invalid character for filename\n",
    "    summary_file = summary_file.replace(':','_')\n",
    "\n",
    "    text = read_file(chapter_file, doc_len)\n",
    "    try:\n",
    "        abst = read_json_file(summary_file, abst_len)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    \n",
    "    doc_data = \"[CLS] \" + text + \" [SEP] \" + abst + \" [SEP]\"\n",
    "    doc_label = pos_label\n",
    "\n",
    "    for j in range(num_neg_ex):\n",
    "        # Write positive example\n",
    "        write_file(test_path + f\"data_{doc_count}.txt\", doc_data)\n",
    "        doc_labels.append(doc_label)\n",
    "        doc_count += 1\n",
    "\n",
    "        # Write negative example\n",
    "        random_element = random.choice(all_file_names)\n",
    "        while random_element == summary_file:\n",
    "            random_element = random.choice(all_file_names)\n",
    "\n",
    "        neg_ex_abst = read_json_file(random_element, abst_len)\n",
    "        neg_ex_data = \"[CLS] \" + text + \" [SEP] \" + neg_ex_abst + \" [SEP]\"\n",
    "        write_file(test_path + f\"data_{doc_count}.txt\", neg_ex_data)\n",
    "        doc_labels.append(neg_label)\n",
    "        doc_count += 1\n",
    "        \n",
    "write_file(test_path + f\"labels.txt\", \"\\n\".join(doc_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "booksum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
