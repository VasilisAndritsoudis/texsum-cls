{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training documents used:\t240384\n",
      "Number of testing documents used:\t9192\n"
     ]
    }
   ],
   "source": [
    "num_train_docs = 287113 + 13368\n",
    "num_test_docs = 11490\n",
    "\n",
    "num_neg_ex = 1\n",
    "\n",
    "docs_per = 0.4\n",
    "\n",
    "num_train_docs = int(2 * num_neg_ex * num_train_docs * docs_per)\n",
    "num_test_docs = int(2 * num_neg_ex * num_test_docs * docs_per)\n",
    "\n",
    "print(f\"Number of training documents used:\\t{num_train_docs}\")\n",
    "print(f\"Number of testing documents used:\\t{num_test_docs}\")\n",
    "\n",
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204326,  36058\n",
      "204326,  36058\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{data_path}/train.csv\", nrows=num_train_docs)\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"{len(train_df)},  {len(val_df)}\")\n",
    "\n",
    "# Extract input, summary, and label for training set\n",
    "train_docs_input = train_df['input'].to_list()\n",
    "train_docs_summary = train_df['summary'].to_list()\n",
    "train_y = train_df['label'].to_list()\n",
    "\n",
    "# Extract input, summary, and label for validation set\n",
    "val_docs_input = val_df['input'].to_list()\n",
    "val_docs_summary = val_df['summary'].to_list()\n",
    "val_y = val_df['label'].to_list()\n",
    "print(f\"{len(train_docs_input)},  {len(val_docs_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation and special characters using regex\n",
    "    tokens = word_tokenize(text) # Tokenize the text\n",
    "    stop_words = set(stopwords.words('english')) # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_input = [preprocess_text(sentence) for sentence in train_docs_input]\n",
    "tokenized_data_summary = [preprocess_text(sentence) for sentence in train_docs_summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_input_val = [preprocess_text(sentence) for sentence in val_docs_input]\n",
    "tokenized_data_summary_val = [preprocess_text(sentence) for sentence in val_docs_summary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized_data = tokenized_data_input + tokenized_data_summary\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=all_tokenized_data, vector_size=embedding_size, window=5, min_count=1, workers=4) # Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Doc Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word):\n",
    "    try:\n",
    "        return word2vec_model.wv[word]\n",
    "    except KeyError:\n",
    "        # Handle out-of-vocabulary words\n",
    "        return np.zeros(embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document embeddings for all documents\n",
    "document_embeddings_input = []\n",
    "\n",
    "for doc in tokenized_data_input:\n",
    "    document_embedding = np.mean([get_word_vector(word) for word in doc], axis=0)\n",
    "    document_embeddings_input.append(document_embedding)\n",
    "\n",
    "document_embeddings_summary = []\n",
    "\n",
    "for doc in tokenized_data_summary:\n",
    "    document_embedding = np.mean([get_word_vector(word) for word in doc], axis=0)\n",
    "    document_embeddings_summary.append(document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings_input = np.array(document_embeddings_input)\n",
    "document_embeddings_summary = np.array(document_embeddings_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document embeddings for all documents\n",
    "document_embeddings_input_val = []\n",
    "\n",
    "for doc in tokenized_data_input_val:\n",
    "    document_embedding = np.mean([get_word_vector(word) for word in doc], axis=0)\n",
    "    document_embeddings_input_val.append(document_embedding)\n",
    "\n",
    "document_embeddings_summary_val = []\n",
    "\n",
    "for doc in tokenized_data_summary_val:\n",
    "    document_embedding = np.mean([get_word_vector(word) for word in doc], axis=0)\n",
    "    document_embeddings_summary_val.append(document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings_input_val = np.array(document_embeddings_input_val)\n",
    "document_embeddings_summary_val = np.array(document_embeddings_summary_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork:\n",
    "    def __init__(self, embedding_size, hidden_layers, learning_rate, num_of_epochs, batch_size):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_of_epochs = num_of_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def euclidean_distance(self, vects):\n",
    "        x, y = vects\n",
    "        sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "        return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "    def eucl_dist_output_shape(self, shapes):\n",
    "        shape1, shape2 = shapes\n",
    "        return (shape1[0], 1)\n",
    "\n",
    "    def build_model(self):\n",
    "        input_a = Input(shape=(self.embedding_size,))\n",
    "        input_b = Input(shape=(self.embedding_size,))\n",
    "        shared_layer = Dense(self.hidden_layers, activation='relu')\n",
    "        encoded_a = shared_layer(input_a)\n",
    "        encoded_b = shared_layer(input_b)\n",
    "        \n",
    "        distance = Lambda(self.euclidean_distance, output_shape=self.eucl_dist_output_shape)([encoded_a, encoded_b])\n",
    "        output = Dense(1, activation='sigmoid')(distance)\n",
    "        model = Model(inputs=[input_a, input_b], outputs=output)\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, document_embeddings_input, document_embeddings_summary, train_y):\n",
    "        # training\n",
    "        input_data = [np.array(document_embeddings_input), np.array(document_embeddings_summary)]\n",
    "        labels_array = np.array(train_y)\n",
    "        self.model.fit(input_data, labels_array, epochs=self.num_of_epochs, batch_size=self.batch_size)\n",
    "\n",
    "    def evaluate(self, document_embeddings, summary_embeddings, labels):\n",
    "\n",
    "        input_data = [np.array(document_embeddings), np.array(summary_embeddings)]\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        #predictions = self.model.predict(input_data)\n",
    "        predictions = self.model.evaluate(input_data, labels)\n",
    "        return predictions\n",
    "    \n",
    "    #def predict(self, document_embeddings, summary_embeddings):\n",
    "\n",
    "        input_data = [np.array(document_embeddings), np.array(summary_embeddings)]\n",
    "\n",
    "        predictions = self.mpdel.predict(document_embeddings, summary_embeddings)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epochs = 20\n",
    "learning_rate = 0.01\n",
    "batch_size = 40\n",
    "hidden_layers = 64\n",
    "# for embedding_size check above  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5109/5109 [==============================] - 18s 2ms/step - loss: 0.6935 - accuracy: 0.5037\n",
      "Epoch 2/20\n",
      "5109/5109 [==============================] - 10s 2ms/step - loss: 0.2490 - accuracy: 0.9008\n",
      "Epoch 3/20\n",
      "5109/5109 [==============================] - 9s 2ms/step - loss: 0.1559 - accuracy: 0.9416\n",
      "Epoch 4/20\n",
      "5109/5109 [==============================] - 9s 2ms/step - loss: 0.1300 - accuracy: 0.9519\n",
      "Epoch 5/20\n",
      "5109/5109 [==============================] - 9s 2ms/step - loss: 0.1184 - accuracy: 0.9567\n",
      "Epoch 6/20\n",
      "5109/5109 [==============================] - 10s 2ms/step - loss: 0.1135 - accuracy: 0.9584\n",
      "Epoch 7/20\n",
      "5109/5109 [==============================] - 10s 2ms/step - loss: 0.1059 - accuracy: 0.9615\n",
      "Epoch 8/20\n",
      "5109/5109 [==============================] - 10s 2ms/step - loss: 0.1040 - accuracy: 0.9620\n",
      "Epoch 9/20\n",
      "5109/5109 [==============================] - 9s 2ms/step - loss: 0.1034 - accuracy: 0.9625\n",
      "Epoch 10/20\n",
      "5109/5109 [==============================] - 10s 2ms/step - loss: 0.1026 - accuracy: 0.9629\n",
      "Epoch 11/20\n",
      "5109/5109 [==============================] - 10s 2ms/step - loss: 0.1022 - accuracy: 0.9628\n",
      "Epoch 12/20\n",
      "5109/5109 [==============================] - 9s 2ms/step - loss: 0.1020 - accuracy: 0.9630\n",
      "Epoch 13/20\n",
      "5109/5109 [==============================] - 9s 2ms/step - loss: 0.1015 - accuracy: 0.9634\n",
      "Epoch 14/20\n",
      "5109/5109 [==============================] - 12s 2ms/step - loss: 0.1015 - accuracy: 0.9632\n",
      "Epoch 15/20\n",
      "5109/5109 [==============================] - 10s 2ms/step - loss: 0.1014 - accuracy: 0.9634\n",
      "Epoch 16/20\n",
      "5109/5109 [==============================] - 12s 2ms/step - loss: 0.1009 - accuracy: 0.9634\n",
      "Epoch 17/20\n",
      "5109/5109 [==============================] - 12s 2ms/step - loss: 0.1006 - accuracy: 0.9637\n",
      "Epoch 18/20\n",
      "5109/5109 [==============================] - 11s 2ms/step - loss: 0.1009 - accuracy: 0.9633\n",
      "Epoch 19/20\n",
      "5109/5109 [==============================] - 13s 3ms/step - loss: 0.1003 - accuracy: 0.9636\n",
      "Epoch 20/20\n",
      "5109/5109 [==============================] - 14s 3ms/step - loss: 0.1005 - accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "siamese_net = SiameseNetwork(embedding_size, hidden_layers, learning_rate, num_of_epochs, batch_size)\n",
    "siamese_net.fit(document_embeddings_input, document_embeddings_summary, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('siamese_net.pkl', 'wb') as outp:\n",
    "    pickle.dump(siamese_net, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model (val set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127/1127 [==============================] - 9s 2ms/step - loss: 0.1056 - accuracy: 0.9605\n",
      "Val Loss: 0.1056\n",
      "Val Accuracy: 96.05%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = siamese_net.evaluate(document_embeddings_input_val, document_embeddings_summary_val, val_y)\n",
    "\n",
    "print(f'Val Loss: {loss:.4f}')\n",
    "print(f'Val Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22980\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(f\"{data_path}/test.csv\", nrows=num_train_docs)\n",
    "\n",
    "print(f\"{len(test_df)}\")\n",
    "\n",
    "# Extract input, summary, and label for testing set\n",
    "test_docs_input = test_df['input'].to_list()\n",
    "test_docs_summary = test_df['summary'].to_list()\n",
    "test_y = test_df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_input_test = [preprocess_text(sentence) for sentence in test_docs_input]\n",
    "tokenized_data_summary_test = [preprocess_text(sentence) for sentence in test_docs_summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document embeddings for all documents\n",
    "document_embeddings_input_test = []\n",
    "\n",
    "for doc in tokenized_data_input_test:\n",
    "    document_embedding = np.mean([get_word_vector(word) for word in doc], axis=0)\n",
    "    document_embeddings_input_test.append(document_embedding)\n",
    "\n",
    "document_embeddings_summary_test = []\n",
    "\n",
    "for doc in tokenized_data_summary_test:\n",
    "    document_embedding = np.mean([get_word_vector(word) for word in doc], axis=0)\n",
    "    document_embeddings_summary_test.append(document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings_input_test = np.array(document_embeddings_input_test)\n",
    "document_embeddings_summary_test = np.array(document_embeddings_summary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/719 [==============================] - 6s 2ms/step - loss: 0.1230 - accuracy: 0.9541\n",
      "Test Loss: 0.1230\n",
      "Test Accuracy: 95.41%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = siamese_net.evaluate(document_embeddings_input_test, document_embeddings_summary_test, test_y)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22980/22980 [==============================] - 42s 2ms/step\n",
      "Loss: 0.12302155047655106, Accuracy: 0.9604873803307223, F1 Score: 0.9610367318915207\n",
      "precision: 0.9478584729981379, recall: 0.9745865970409051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "predictions = siamese_net.model.predict([np.array(document_embeddings_input_test), np.array(document_embeddings_summary_test)], 1)\n",
    "#predictions = siamese_net.predict(document_embeddings_input_test, document_embeddings_summary_test)\n",
    "\n",
    "# Convert predicted probabilities to binary predictions (0 or 1)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_y, binary_predictions)\n",
    "ac = accuracy_score(test_y, binary_predictions)\n",
    "precision = precision_score(test_y, binary_predictions)\n",
    "recall = recall_score(test_y, binary_predictions)\n",
    "\n",
    "print(f'Loss: {loss}, Accuracy: {ac}, F1 Score: {f1}')\n",
    "print(f'precision: {precision}, recall: {recall}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
