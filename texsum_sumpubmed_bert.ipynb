{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:31.602351700Z",
     "start_time": "2023-12-30T16:30:07.548098200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80997f36d79e8a9e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "num_docs = 32689\n",
    "num_neg_ex = 2\n",
    "\n",
    "test_docs_per = 0.1\n",
    "dataset_usage_per = 0.005\n",
    "\n",
    "num_test_docs = int(2 * num_neg_ex * num_docs * test_docs_per * dataset_usage_per) + 1\n",
    "num_train_docs = int((2 * num_neg_ex * num_docs - num_test_docs) * dataset_usage_per)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:31.791350100Z",
     "start_time": "2023-12-30T16:30:31.595351700Z"
    }
   },
   "id": "7e74d91e8d9baa1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "126ea5520922f15f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "num_of_epochs = 2\n",
    "learning_rate = 27e-6\n",
    "batch_size = 16\n",
    "hidden_layers = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:31.793355Z",
     "start_time": "2023-12-30T16:30:31.627351800Z"
    }
   },
   "id": "3c6c2641c1c133ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "625d47f0772920e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_doc(path):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "    return data\n",
    "    \n",
    "def read_labels(path):\n",
    "    labels = []\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        count = 1\n",
    "        for line in f:\n",
    "            if count > num_train_docs:\n",
    "                break\n",
    "            labels.append(float(line.rstrip()))\n",
    "            count += 1\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:31.868351200Z",
     "start_time": "2023-12-30T16:30:31.643352400Z"
    }
   },
   "id": "c1c57bbb397cca3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Dataset\n",
    "Because of the large size of the Dataset, only the document indices will be used for splitting into train/validation sets. The documents will be loaded when the tokenization process takes place."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64ec637cb99b55b9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "indices = [i for i in range(1, num_train_docs+1)]\n",
    "labels = read_labels(f\"data\\\\raw\\\\train\\\\labels.txt\")\n",
    "\n",
    "train_indices, val_indices, train_y, val_y = train_test_split(indices, labels, test_size=0.15, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:31.897351100Z",
     "start_time": "2023-12-30T16:30:31.671352Z"
    }
   },
   "id": "cdabb857f19bc585"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "Each document is loaded from the disk and then run through the tokenizer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9da6597fe4e09e7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "pretrained_model = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model)\n",
    "\n",
    "# Tokenize train data\n",
    "documents = []\n",
    "for i in train_indices:\n",
    "    text = read_doc(f\"data\\\\raw\\\\train\\\\data_{i}.txt\")\n",
    "    documents.append(text)\n",
    "train_X = tokenizer(documents, max_length=512, truncation='longest_first', return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "# Tokenize validation data\n",
    "documents = []\n",
    "for i in val_indices:\n",
    "    text = read_doc(f\"data\\\\raw\\\\train\\\\data_{i}.txt\")\n",
    "    documents.append(text)\n",
    "val_X = tokenizer(documents, max_length=512, truncation='longest_first', return_tensors=\"pt\")\n",
    "\n",
    "del documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:35.402922Z",
     "start_time": "2023-12-30T16:30:31.742351200Z"
    }
   },
   "id": "9381b8bd0e982aaf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom PyTorch Dataset\n",
    "Create a custom PyTorch Dataset that will contain the tokenized text encodings. Then use DataLoaders to prepare the Dataset for training and testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82b6e2e57533124f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class SumPubMedDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # an encoding can have keys such as input_ids and attention_mask\n",
    "        # item is a dictionary which has the same keys as the encoding has\n",
    "        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:35.427920900Z",
     "start_time": "2023-12-30T16:30:35.410927100Z"
    }
   },
   "id": "4d403fa44b58dd41"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_dataset = SumPubMedDataset(train_X, train_y)\n",
    "val_dataset = SumPubMedDataset(val_X, val_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:35.478919600Z",
     "start_time": "2023-12-30T16:30:35.423924400Z"
    }
   },
   "id": "ac43878b3886d0f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4eaa236f62a8836"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "in_features = 768 # it's 768 because that's the size of the output provided by the underlying BERT model\n",
    "\n",
    "class TexSumClassifier(torch.nn.Module):\n",
    "    def __init__(self, linear_size):\n",
    "        super(TexSumClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout1 = torch.nn.Dropout()\n",
    "        self.linear1 = torch.nn.Linear(in_features=in_features, out_features=linear_size)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(num_features=linear_size)\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.8)\n",
    "        self.linear2 = torch.nn.Linear(in_features=linear_size, out_features=1)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(num_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, tokens, attention_mask):\n",
    "        bert_output = self.bert(input_ids=tokens, attention_mask=attention_mask)\n",
    "        x = self.dropout1(bert_output[1])\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def freeze_bert(self):\n",
    "        \"\"\"\n",
    "        Freezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        only the wieghts of the custom classifier are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.named_parameters():\n",
    "            param[1].requires_grad=False\n",
    "    \n",
    "    def unfreeze_bert(self):\n",
    "        \"\"\"\n",
    "        Unfreezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        both the weights of the custom classifier and of the underlying BERT are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.named_parameters():\n",
    "            param[1].requires_grad=True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:35.524922700Z",
     "start_time": "2023-12-30T16:30:35.440923400Z"
    }
   },
   "id": "6b64e2de886ca52c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56e52999e3215212"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def eval_prediction(y_batch_actual, y_batch_predicted):\n",
    "    \"\"\"Return batches of accuracy and f1 scores.\"\"\"\n",
    "    y_batch_actual_np = y_batch_actual.cpu().detach().numpy()\n",
    "    y_batch_predicted_np = np.round(y_batch_predicted.cpu().detach().numpy())\n",
    "    \n",
    "    acc = accuracy_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np)\n",
    "    f1 = f1_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np, average='weighted')\n",
    "    \n",
    "    return acc, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:35.595921500Z",
     "start_time": "2023-12-30T16:30:35.523921400Z"
    }
   },
   "id": "a7b20faec73a8760"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bb20a30407ed0cf"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "E:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\transformers\\optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = TexSumClassifier(linear_size=hidden_layers)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:40.805968900Z",
     "start_time": "2023-12-30T16:30:35.566921500Z"
    }
   },
   "id": "46cc73b4c7cee294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training step"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44a5a4cac32c29d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def training_step(dataloader, model, optimizer, loss_fn, if_freeze_bert):\n",
    "    \"\"\"Method to train the model\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    model.freeze_bert() if if_freeze_bert else model.unfreeze_bert()\n",
    "      \n",
    "    epoch_loss = 0\n",
    " \n",
    "    for i, batch in enumerate(dataloader):        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        outputs = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n",
    "                        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:40.875969100Z",
     "start_time": "2023-12-30T16:30:40.802968800Z"
    }
   },
   "id": "78ffde4cee8adaae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation step"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a72b9a35f7f93521"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def validation_step(dataloader, model, loss_fn):\n",
    "    \"\"\"Method to test the model's accuracy and loss on the validation set\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    model.freeze_bert()\n",
    "    \n",
    "    size = len(dataloader)\n",
    "    f1, acc = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            y = batch['labels'].to(device)\n",
    "                  \n",
    "            pred = model(tokens=X, attention_mask=attention_mask)\n",
    "            \n",
    "            acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n",
    "            acc += acc_batch\n",
    "            f1 += f1_batch\n",
    "\n",
    "        acc = acc/size\n",
    "        f1 = f1/size\n",
    "                \n",
    "    return acc, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:30:40.921968Z",
     "start_time": "2023-12-30T16:30:40.827971200Z"
    }
   },
   "id": "ba23e02c8f5192ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd5690c1713bcaf1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fef0fe248d243f2a397439e7fc44027"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: #1\n",
      "Bert is not frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 2.00 GiB total capacity; 5.00 GiB already allocated; 0 bytes free; 5.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12936\\819848447.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Bert is frozen\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m     \u001B[0mtraining_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mif_freeze_bert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m     \u001B[0mtrain_acc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_f1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidation_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[0mval_acc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_f1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidation_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12936\\1436760055.py\u001B[0m in \u001B[0;36mtraining_step\u001B[1;34m(dataloader, model, optimizer, loss_fn, if_freeze_bert)\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'labels'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12936\\3497557832.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, tokens, attention_mask)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokens\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m         \u001B[0mbert_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbert_output\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1028\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1029\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1030\u001B[1;33m             \u001B[0mreturn_dict\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1031\u001B[0m         )\n\u001B[0;32m   1032\u001B[0m         \u001B[0msequence_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    615\u001B[0m                     \u001B[0mencoder_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    616\u001B[0m                     \u001B[0mpast_key_value\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 617\u001B[1;33m                     \u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    618\u001B[0m                 )\n\u001B[0;32m    619\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    498\u001B[0m             \u001B[0mhead_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    499\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 500\u001B[1;33m             \u001B[0mpast_key_value\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself_attn_past_key_value\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    501\u001B[0m         )\n\u001B[0;32m    502\u001B[0m         \u001B[0mattention_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself_attention_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    430\u001B[0m             \u001B[0mencoder_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    431\u001B[0m             \u001B[0mpast_key_value\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 432\u001B[1;33m             \u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    433\u001B[0m         )\n\u001B[0;32m    434\u001B[0m         \u001B[0mattention_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    355\u001B[0m         \u001B[1;31m# This is actually dropping out entire tokens to attend to, which might\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    356\u001B[0m         \u001B[1;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 357\u001B[1;33m         \u001B[0mattention_probs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_probs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    358\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    359\u001B[0m         \u001B[1;31m# Mask heads if we want to\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\User\\Desktop\\texsum-cls\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mdropout\u001B[1;34m(input, p, training, inplace)\u001B[0m\n\u001B[0;32m   1250\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0.0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1251\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"dropout probability has to be between 0 and 1, \"\u001B[0m \u001B[1;34m\"but got {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1252\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0minplace\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0m_VF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1253\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1254\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 2.00 GiB total capacity; 5.00 GiB already allocated; 0 bytes free; 5.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "best_acc, best_f1 = 0, 0\n",
    "path = \"models\\\\sumpubmed_bert\\\\best_model.pt\"\n",
    "if_freeze_bert = False\n",
    "\n",
    "for i in tqdm(range(num_of_epochs)):\n",
    "    print(\"Epoch: #{}\".format(i+1))\n",
    "\n",
    "    if i < 5:\n",
    "        if_freeze_bert = False\n",
    "        print(\"Bert is not frozen\")\n",
    "    else:\n",
    "        if_freeze_bert = True\n",
    "        print(\"Bert is frozen\")\n",
    "    \n",
    "    training_step(train_loader, model,optimizer, loss_fn, if_freeze_bert)\n",
    "    train_acc, train_f1 = validation_step(train_loader, model, loss_fn)\n",
    "    val_acc, val_f1 = validation_step(val_loader, model, loss_fn)\n",
    "    \n",
    "    print(\"Training results: \")\n",
    "    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n",
    "    \n",
    "    print(\"Validation results: \")\n",
    "    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc    \n",
    "        torch.save(model, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T16:31:08.514395100Z",
     "start_time": "2023-12-30T16:30:40.863970500Z"
    }
   },
   "id": "f418bf371314eac7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fd24d35a97d186e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Test Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d244eebea6ec582e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_indices = [i for i in range(1, num_test_docs+1)]\n",
    "test_y = read_labels(f\"data\\\\raw\\\\test\\\\labels.txt\")\n",
    "\n",
    "# Tokenize train data\n",
    "test_X = []\n",
    "for i in test_indices:\n",
    "    text = read_doc(f\"data\\\\raw\\\\test\\\\data_{i}.txt\")\n",
    "    encodings = tokenizer(text, max_length=512, truncation='longest_first', return_tensors=\"pt\")\n",
    "    test_X.append(encodings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-30T16:31:08.361689700Z"
    }
   },
   "id": "56f14111ef6b133d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate on Test Predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3147eea1fe4aa56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = torch.load(path)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens=test_X['input_ids'], attention_mask=test_X['attention_mask'])\n",
    "    acc_test, f1_test = eval_prediction(test_y.float(), predictions) \n",
    "binary_predictions = np.round(predictions.cpu().detach().numpy()).astype(int).flatten()\n",
    "    \n",
    "print(\"Testing results: \")\n",
    "print(\"Acc: {:.3f}, f1: {:.3f}\".format(acc_test, f1_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-30T16:31:08.368689300Z"
    }
   },
   "id": "98b77b48f6bde67b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
